"""
æ–‡æœ¬ç‰¹å¾æå–å™¨ï¼šä»åŸå§‹æ•°æ®ä¸­æå–å‡ºæ¨¡å‹å¯ç”¨çš„æ–‡æœ¬ç‰¹å¾
"""

import pandas as pd
import numpy as np
import re
import jieba
import jieba.posseg as pseg
import json
import os

# ================= è¯åº“æ³¨å†Œ (LexiconRegistry) =================
class LexiconRegistry:
    """
    è´Ÿè´£åŠ è½½ README.md ä¸­æè¿°çš„ JSON æ ¼å¼è¯åº“
    """
    def __init__(self, base_dir='lexicons'):
        self.base_dir = base_dir
        
        # --- é›†åˆç±»è¯åº“ (Set) ---
        self.ingredients = set()       # æˆåˆ†
        self.efficacy = set()          # åŠŸæ•ˆ
        self.product_categories = set()# å“ç±»
        self.skin_types = set()        # è‚¤è´¨
        self.colloquial = set()        # å£è¯­
        self.emotions = set()          # æƒ…ç»ª
        self.hot_words = set()         # çƒ­è¯
        self.audiences = set()         # äººç¾¤
        self.pain_points = set()       # ç—›ç‚¹
        self.scenarios = set()         # åœºæ™¯
        self.budget_sensitive = set()  # é¢„ç®—æ•æ„Ÿ
        self.vertical_tags = set()     # å‚ç›´æ ‡ç­¾ (ç¼ºæ–‡ä»¶)
        self.generic_tags = set()      # æ³›æ ‡ç­¾ (ç¼ºæ–‡ä»¶)
        self.search_keywords_global = set() # å…¨å±€æ£€ç´¢è¯åº“
        
        # --- æ­£åˆ™æ¨¡å¼ç±» (List) ---
        self.imperative_patterns = []  # å·å¬
        self.budget_patterns = []      # ä»·æ ¼æ­£åˆ™
        
        # --- ç»“æ„åŒ–æ¨¡å¼ (æ¥è‡ª structure_patterns.json) ---
        self.usage_patterns = []       # ç”¨æ³•/æ­¥éª¤
        self.summary_patterns = []     # æ€»ç»“
        self.comparison_patterns = []  # å¯¹æ¯”
        self.pain_solution_patterns = [] # ç—›ç‚¹æ–¹æ¡ˆé“¾
        
        # --- åŠ è½½è¿‡ç¨‹ (éœ€ç¡®ä¿æ–‡ä»¶å­˜åœ¨) ---
        print(f">>> æ­£åœ¨åŠ è½½è¯åº“ (base_dir={self.base_dir})...")
        try:
            # 1. åŠ è½½å„ç±» Term (Standard JSON)
            self._load_terms('beauty_knowledge/ingredients.json', self.ingredients)
            self._load_terms('beauty_knowledge/efficacy.json', self.efficacy)
            self._load_terms('beauty_knowledge/product_category.json', self.product_categories)
            self._load_terms('beauty_knowledge/skin_type.json', self.skin_types)
            self._load_terms('content_style/colloquial.json', self.colloquial)
            self._load_terms('content_style/emotion.json', self.emotions) # æ³¨æ„æ–‡ä»¶åæ˜¯ emotion.json
            self._load_terms('content_style/hotwords.json', self.hot_words)
            self._load_terms('user_context/audience.json', self.audiences)
            self._load_terms('user_context/painpoint.json', self.pain_points)
            self._load_terms('user_context/scenario.json', self.scenarios)
            self._load_terms('user_context/budget.json', self.budget_sensitive)
            
            # 2. åŠ è½½æ­£åˆ™ Patterns
            self._load_simple_patterns('patterns/imperative_patterns.json', self.imperative_patterns)
            self._load_simple_patterns('user_context/budget.json', self.budget_patterns, key='price_patterns')
            
            # 3. åŠ è½½å¤æ‚çš„ç»“æ„åŒ– Patterns
            self._load_structure_patterns('patterns/structure_patterns.json')

            # 4. åŠ è½½å…¨å±€æ£€ç´¢è¯ (search_keywords.json åŒ…å« groups)
            self._load_search_keywords('search/search_keywords.json')
            
            # 5. Jieba åˆå§‹åŒ–
            all_words = self.ingredients | self.efficacy | self.pain_points | self.hot_words
            for w in all_words:
                jieba.add_word(w)
                
            print(f"è¯åº“åŠ è½½å®Œæ¯•ã€‚æˆåˆ†è¯: {len(self.ingredients)}, åŠŸæ•ˆè¯: {len(self.efficacy)}")
            
        except Exception as e:
            print(f"Error loading lexicons: {e}")
            import traceback
            traceback.print_exc()

    def _load_terms(self, rel_path, target_set):
        path = os.path.join(self.base_dir, rel_path)
        if not os.path.exists(path): 
            print(f"Warning: File not found {path}")
            return
        try:
            with open(path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                # å‡è®¾æ ‡å‡†æ ¼å¼: { "terms": [ {"term":...}, ... ] }
                if 'terms' in data and isinstance(data['terms'], list):
                    for item in data['terms']:
                        if 'term' in item: target_set.add(item['term'])
                        if 'synonyms' in item: target_set.update(item['synonyms'])
        except Exception as e:
            print(f"Failed to load terms from {rel_path}: {e}")

    def _load_simple_patterns(self, rel_path, target_list, key='patterns'):
        """åŠ è½½ç®€å•çš„æ­£åˆ™åˆ—è¡¨ï¼Œå¦‚ imperative_patterns"""
        path = os.path.join(self.base_dir, rel_path)
        if not os.path.exists(path): return
        try:
            with open(path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                # å¯èƒ½æ˜¯ {"patterns": [...]} æˆ– {"price_patterns": [...]}
                patterns = data.get(key, [])
                for p in patterns:
                    if isinstance(p, dict) and 'regex' in p:
                        target_list.append(p['regex'])
                    elif isinstance(p, str): # å…¼å®¹çº¯å­—ç¬¦ä¸²åˆ—è¡¨
                        target_list.append(p)
        except Exception as e:
            print(f"Failed to load patterns from {rel_path}: {e}")

    def _load_structure_patterns(self, rel_path):
        """ä¸“é—¨åŠ è½½ structure_patterns.json çš„å¤æ‚ç»“æ„"""
        path = os.path.join(self.base_dir, rel_path)
        if not os.path.exists(path): return
        try:
            with open(path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                # data['patterns'] æ˜¯ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å« usage_method, summary, comparison ç­‰
                root = data.get('patterns', {})
                
                # Helper to extract regex list from a group
                def extract(group_name):
                    res = []
                    items = root.get(group_name, [])
                    for item in items:
                        if 'regex' in item: res.append(item['regex'])
                    return res

                self.usage_patterns = extract('usage_method')
                self.summary_patterns = extract('summary')
                self.comparison_patterns = extract('comparison')
                self.pain_solution_patterns = extract('painpoint_solution_effect')
                
        except Exception as e:
            print(f"Failed to load structure patterns: {e}")

    def _load_search_keywords(self, rel_path):
        """åŠ è½½ search_keywords.json (å¸¦ groups ç»“æ„)"""
        path = os.path.join(self.base_dir, rel_path)
        if not os.path.exists(path): return
        try:
            with open(path, 'r', encoding='utf-8') as f:
                content = json.load(f)
                groups = content.get('groups', {})
                for group_key, group_val in groups.items():
                    if 'terms' in group_val:
                        for t in group_val['terms']:
                            self.search_keywords_global.add(t['term'])
                            if 'synonyms' in t: self.search_keywords_global.update(t['synonyms'])
        except Exception as e:
            print(f"Failed to load search keywords: {e}")

# ================= ç‰¹å¾æå–å™¨ (FeatureExtractor) =================
class FeatureExtractor:
    def __init__(self, lexicons):
        self.lex = lexicons

    # --- é€šç”¨å·¥å…·å‡½æ•° ---
    def _count_hits(self, text, lexicon):
        return sum(1 for word in lexicon if word in text) if text else 0

    def _count_unique_hits(self, text, lexicon):
        return len(set(word for word in lexicon if word in text)) if text else 0

    def _check_regex(self, text, patterns):
        if not text: return 0
        for pat in patterns:
            try:
                if re.search(pat, text, re.IGNORECASE): return 1
            except re.error:
                continue
        return 0

    # ================= æ¨¡å— A: æ ‡é¢˜ç‰¹å¾ (Row 0-6) =================
    def extract_title_features(self, row):
        title = str(row.get('title', ''))
        search_kw = str(row.get('search_keyword', ''))
        
        feats = {}
        # [Def] Row 0: æ ‡é¢˜é•¿åº¦ï¼ˆå­—ç¬¦æ•°ï¼‰
        feats['title_len'] = len(title)
        
        # [Def] Row 1: æ ‡é¢˜æ˜¯å¦åŒ…å«æ•°å­—
        feats['title_number_flag'] = 1 if re.search(r'\d|[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]', title) else 0
        
        # [Def] Row 2: æ ‡é¢˜æ˜¯å¦ä¸ºç–‘é—®å¥
        feats['title_question_flag'] = 1 if re.search(r'[?ï¼Ÿ]|æ€ä¹ˆ|å¦‚ä½•|å¥½ç”¨å—|ä»€ä¹ˆ|é¿é›·å—', title) else 0
        
        # [Def] Row 3: æ ‡é¢˜å‘½ä¸­æ ¸å¿ƒæ£€ç´¢è¯è¦†ç›–ç‡ (0 æˆ– 1)
        feats['title_keyword_cov'] = 1 if (search_kw and search_kw in title) else 0
        
        # [Def] Row 4: æ ‡é¢˜å‘½ä¸­æ ¸å¿ƒæ£€ç´¢è¯æ•°é‡ (å…¨åº“èŒƒå›´)
        feats['title_keyword_cnt'] = self._count_hits(title, self.lex.search_keywords_global)
        
        # [Def] Row 5: æ ¸å¿ƒæ£€ç´¢è¯åœ¨æ ‡é¢˜ä¸­çš„é å‰ç¨‹åº¦ (ä½ç½®å½’ä¸€åŒ–: 1æœ€å‰, 0æœ€å)
        if search_kw and search_kw in title:
            pos = title.find(search_kw)
            feats['title_keyword_pos_score'] = 1 - (pos / len(title))
        else:
            feats['title_keyword_pos_score'] = 0
            
        # [Def] Row 6: æ ‡é¢˜å¯è¯»æ€§ (ç®€åŒ–ï¼šç¬¦å·å æ¯”è¶Šä½ï¼Œå¯è¯»æ€§è¶Šé«˜)
        symbol_cnt = len(re.findall(r'[^\w\s]', title))
        feats['title_readability_score'] = 1 - (symbol_cnt / (len(title) + 1))
        
        return feats

    # ================= æ¨¡å— B: æ­£æ–‡ç»“æ„ç‰¹å¾ (Row 7-15) =================
    def extract_content_features(self, row):
        desc = str(row.get('desc', ''))
        feats = {}
        
        # [Def] Row 7: æ­£æ–‡é•¿åº¦
        feats['content_len'] = len(desc)
        
        # [Def] Row 8: æ­£æ–‡å¥å­æ•° (æŒ‰æ ‡ç‚¹åˆ‡åˆ†)
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ.!?\n]', desc)
        sentences = [s for s in sentences if len(s.strip()) > 1] # è¿‡æ»¤ç©ºå¥
        feats['sentence_cnt'] = len(sentences)
        
        # [Def] Row 9: å¹³å‡å¥é•¿
        feats['avg_sentence_len'] = np.mean([len(s) for s in sentences]) if sentences else 0
        
        # [Def] Row 10: æ­£æ–‡æ®µè½æ•° (æŒ‰æ¢è¡Œåˆ‡åˆ†)
        feats['paragraph_cnt'] = desc.count('\n') + 1
        
        # [Def] Row 11: æ˜¯å¦å‘ˆç°åˆ—è¡¨ç»“æ„ (æ­£åˆ™æ£€æµ‹ 1. 2. æˆ– emojiåˆ—è¡¨)
        list_pat = r'(\d\.|[abcd]\.|â€¢|âœ”|âœ…|ğŸ‘‰|â‘ |â‘¡)'
        matches = re.findall(list_pat, desc)
        feats['list_structure_flag'] = 1 if len(matches) >= 3 else 0
        
        # [Def] Row 12: æ˜¯å¦åŒ…å«æ€»ç»“æ®µè½ (ä½¿ç”¨ structure_patterns)
        # å¦‚æœ patterns ä¸ºç©ºï¼Œå›é€€åˆ°ç¡¬ç¼–ç 
        if self.lex.summary_patterns:
            feats['summary_flag'] = self._check_regex(desc, self.lex.summary_patterns)
        else:
            feats['summary_flag'] = self._check_regex(desc, [r'(æ€»ç»“|ç»¼ä¸Š|ç»“è®º|æœ€å|æ€»çš„æ¥è¯´)'])
        
        # [Def] Row 13: ä¿¡æ¯å¯†åº¦ (å®è¯/æ€»è¯æ•°)
        try:
            words = list(pseg.cut(desc))
            content_words = [w for w, flag in words if flag.startswith(('n', 'v', 'a'))]
            feats['info_density_score'] = len(content_words) / (len(words) + 1)
        except:
            feats['info_density_score'] = 0
            
        # [Def] Row 14: æ­£æ–‡å¯è¯»æ€§
        avg_len = feats['avg_sentence_len']
        feats['readability_score'] = max(0, min(1, 1 - (avg_len - 5) / 45))
        
        # [Def] Row 15: ç—›ç‚¹â†’æ–¹æ¡ˆâ†’æ•ˆæœç»“æ„
        # ä¼˜å…ˆä½¿ç”¨ patternsï¼Œå¦‚æœæ²¡å‘½ä¸­å†ç”¨è¯åº“å…±ç°å…œåº•
        pat_flag = self._check_regex(desc, self.lex.pain_solution_patterns)
        if pat_flag:
            feats['solution_pattern_flag'] = 1
        else:
            has_pain = 1 if self._count_hits(desc, self.lex.pain_points) > 0 else 0
            has_eff = 1 if self._count_hits(desc, self.lex.efficacy) > 0 else 0
            feats['solution_pattern_flag'] = 1 if (has_pain and has_eff) else 0
        
        return feats

    # ================= æ¨¡å— C: è¯­ä¹‰ç‰¹å¾ (Row 16-39) =================
    def extract_semantic_features(self, row):
        full_text = str(row.get('title', '')) + " " + str(row.get('desc', ''))
        feats = {}
        
        # [Def] Row 16: çƒ­è¯å‘½ä¸­ç‡
        hits = self._count_hits(full_text, self.lex.hot_words)
        feats['hotword_hit_rate'] = hits / (len(full_text) / 100 + 1)
        
        # [Def] Row 17: å£è¯­åŒ–æ¯”ä¾‹
        feats['colloquial_ratio'] = self._count_hits(full_text, self.lex.colloquial) / (len(full_text)/100 + 1)
        
        # [Def] Row 18: Emoji å æ¯”
        emoji_cnt = len(re.findall(r'\[.*?\]', full_text)) 
        feats['emoji_ratio'] = emoji_cnt / (len(full_text) + 1)
        
        # [Def] Row 19: æ ‡ç‚¹å¯†åº¦
        punct_cnt = len(re.findall(r'[ï¼Œã€‚ï¼ï¼Ÿã€,!?]', full_text))
        feats['punctuation_density'] = punct_cnt / (len(full_text) + 1)
        
        # [Def] Row 20: æ„Ÿå¹å·å æ¯”
        feats['exclamation_ratio'] = (full_text.count('!') + full_text.count('ï¼')) / (len(full_text) + 1)
        
        # [Def] Row 21: é—®å·å æ¯”
        feats['question_ratio'] = (full_text.count('?') + full_text.count('ï¼Ÿ')) / (len(full_text) + 1)
        
        # [Def] Row 22: ç¬¬äºŒäººç§°å æ¯”
        sec_person_words = ['ä½ ', 'ä½ ä»¬', 'å§å¦¹', 'å®å®', 'å¤§å®¶', 'é›†ç¾']
        feats['second_person_ratio'] = sum(full_text.count(w) for w in sec_person_words) / (len(full_text)/100 + 1)
        
        # [Def] Row 23: ç¥ˆä½¿/å·å¬è¡¨è¾¾
        feats['imperative_ratio'] = self._check_regex(full_text, self.lex.imperative_patterns)
        
        # [Def] Row 24: æƒ…ç»ªå¼ºåº¦
        feats['sentiment_intensity'] = feats['exclamation_ratio'] * 100 + self._count_hits(full_text, self.lex.emotions)
        
        # [Def] Row 25-26: æˆåˆ†è¯
        feats['ingredient_cnt'] = self._count_hits(full_text, self.lex.ingredients)
        feats['ingredient_diversity'] = self._count_unique_hits(full_text, self.lex.ingredients)
        
        # [Def] Row 27-28: åŠŸæ•ˆè¯
        feats['efficacy_cnt'] = self._count_hits(full_text, self.lex.efficacy)
        feats['efficacy_diversity'] = self._count_unique_hits(full_text, self.lex.efficacy)
        
        # [Def] Row 29: è‚¤è´¨è¯
        feats['skin_type_cnt'] = self._count_hits(full_text, self.lex.skin_types)
        
        # [Def] Row 30: å“ç±»è¯
        feats['product_category_cnt'] = self._count_hits(full_text, self.lex.product_categories)
        
        # [Def] Row 31: ç”¨æ³•/æ­¥éª¤ä¿¡æ¯ (ä½¿ç”¨ç»“æ„åŒ– Patterns)
        feats['usage_method_flag'] = self._check_regex(full_text, self.lex.usage_patterns)
        
        # [Def] Row 32: å¯¹æ¯”/å‰åæ•ˆæœ (ä½¿ç”¨ç»“æ„åŒ– Patterns)
        if self.lex.comparison_patterns:
            feats['comparison_flag'] = self._check_regex(full_text, self.lex.comparison_patterns)
        else:
            feats['comparison_flag'] = 1 if re.search(r'(å¯¹æ¯”|åŒºåˆ«|PK|pk|èƒœå‡º|å‰å|å˜åŒ–)', full_text) else 0
        
        # [Def] Row 33-35: äººç¾¤/åœºæ™¯/ç—›ç‚¹
        feats['audience_word_cnt'] = self._count_hits(full_text, self.lex.audiences)
        feats['scenario_word_cnt'] = self._count_hits(full_text, self.lex.scenarios)
        feats['painpoint_word_cnt'] = self._count_hits(full_text, self.lex.pain_points)
        
        # [Def] Row 36: ä»·æ ¼æ•æ„Ÿ
        feats['budget_sensitivity_flag'] = 1 if (
            self._count_hits(full_text, self.lex.budget_sensitive) > 0 or 
            self._check_regex(full_text, self.lex.budget_patterns)
        ) else 0
        
        # [Def] Row 37-39: å…¨æ–‡æ£€ç´¢è¯ç‰¹å¾
        feats['search_keyword_cov'] = 1 if str(row.get('search_keyword')) in full_text else 0
        feats['search_keyword_cnt'] = self._count_hits(full_text, self.lex.search_keywords_global)
        feats['keyword_density'] = feats['search_keyword_cnt'] / (len(full_text) + 1)
        
        return feats

    # ================= æ¨¡å— D: æ ‡ç­¾ç‰¹å¾ (Row 40-44) =================
    def extract_tag_features(self, row):
        tags_str = str(row.get('tags', ''))
        tags = [t.strip().replace('#', '').replace('[è¯é¢˜]', '') for t in re.split(r'[,ï¼Œ\s]', tags_str) if t.strip()]
        
        feats = {}
        
        # [Def] Row 40: æ ‡ç­¾ä¸æ­£æ–‡ä¸€è‡´æ€§
        desc = str(row.get('desc', ''))
        overlap = sum(1 for t in tags if t in desc)
        feats['tag_content_consistency'] = overlap / len(tags) if tags else 0
        
        # [Def] Row 41: è¯é¢˜æ ‡ç­¾æ•°é‡
        feats['tag_cnt'] = len(tags)
        
        # [Def] Row 42: å‚ç›´æ ‡ç­¾å æ¯” (ç¼ºå¤±è¯åº“ï¼Œæš‚ä¸º0)
        v_hits = sum(1 for t in tags if t in self.lex.vertical_tags)
        feats['vertical_tag_ratio'] = v_hits / len(tags) if tags else 0
        
        # [Def] Row 43: æ³›æ ‡ç­¾å æ¯” (ç¼ºå¤±è¯åº“ï¼Œæš‚ä¸º0)
        g_hits = sum(1 for t in tags if t in self.lex.generic_tags)
        feats['generic_tag_ratio'] = g_hits / len(tags) if tags else 0
        
        # [Def] Row 44: æ ‡ç­¾ä¸­æ ¸å¿ƒæ£€ç´¢è¯å‘½ä¸­æ•°
        k_hits = sum(1 for t in tags if t in self.lex.search_keywords_global)
        feats['tag_keyword_hit_cnt'] = k_hits
        
        return feats

    # ================= ä¸»æµç¨‹ =================
    def process(self, df):
        print(">>> å¼€å§‹ç‰¹å¾æå– (Rows 0-44)...")
        results = []
        for idx, row in df.iterrows():
            f_row = {}
            f_row.update(self.extract_title_features(row))
            f_row.update(self.extract_content_features(row))
            f_row.update(self.extract_semantic_features(row))
            f_row.update(self.extract_tag_features(row))
            results.append(f_row)
            
        feat_df = pd.DataFrame(results)
        meta_cols = ['note_id', 'title', 'hot_level', 'search_keyword']
        final = pd.concat([df[meta_cols], feat_df], axis=1)
        return final

# ================= 3. è¿è¡Œè„šæœ¬ =================
if __name__ == "__main__":
    # 1. å‡†å¤‡æ•°æ®
    try:
        # å‡è®¾è¿è¡Œåœ¨æ ¹ç›®å½•ä¸‹
        input_file = 'data/data_with_label.csv'
        if not os.path.exists(input_file):
            # å…¼å®¹ src ç›®å½•è¿è¡Œ
            input_file = '../data/data_with_label.csv'
            
        df = pd.read_csv(input_file)
        print(f"è½½å…¥æ•°æ®: {len(df)} æ¡")
        
        # 2. åˆå§‹åŒ–è¯åº“
        # è‡ªåŠ¨æ¢æµ‹è¯åº“ç›®å½•
        lex_dir = 'lexicons'
        if not os.path.exists(lex_dir):
            lex_dir = '../lexicons'
            
        registry = LexiconRegistry(base_dir=lex_dir)
        
        # 3. æå–ç‰¹å¾
        extractor = FeatureExtractor(registry)
        df_features = extractor.process(df)
        
        # 4. ç»“æœæ£€æŸ¥ä¸ä¿å­˜
        print("\nç‰¹å¾æå–å®Œæˆã€‚ç»“æœé¢„è§ˆ:")
        print(df_features[['title_len', 'info_density_score', 'ingredient_cnt', 'hot_level']].head())
        
        output_file = 'data/data_with_text_features.csv'
        df_features.to_csv(output_file, index=False, encoding='utf-8-sig')
        print(f"\næ–‡ä»¶å·²ä¿å­˜è‡³: {output_file}")
        
    except FileNotFoundError as e:
        print(f"æ–‡ä»¶ç¼ºå¤±: {e}")
    except Exception as e:
        print(f"è¿è¡Œæ—¶é”™è¯¯: {e}")
