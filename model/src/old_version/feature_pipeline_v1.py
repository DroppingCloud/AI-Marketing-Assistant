"""
æ—§ç‰ˆç‰¹å¾æå–å™¨ - ç»Ÿä¸€æ–‡æœ¬ä¸è§†è§‰ç‰¹å¾æå–
====================================

åŠŸèƒ½æ¦‚è¿°:
---------
1. æ–‡æœ¬ç‰¹å¾æå–: ä»æ ‡é¢˜ã€æ­£æ–‡ã€æ ‡ç­¾ä¸­æå–45ç»´æ–‡æœ¬ç‰¹å¾
2. è§†è§‰ç‰¹å¾æå–: ä»å›¾ç‰‡ä¸­æå–10ç»´è§†è§‰ç‰¹å¾
3. æ‰¹é‡å¤„ç†: æ”¯æŒä»CSVè¯»å–æ•°æ®ï¼ŒåŒ¹é…å›¾ç‰‡ï¼Œè¾“å‡ºå®Œæ•´ç‰¹å¾é›†
"""

import pandas as pd
import numpy as np
import cv2
import re
import os
import math
import json
import jieba
import jieba.posseg as pseg
from tqdm import tqdm

# ============================================================================
# å¯é€‰ä¾èµ–æ£€æµ‹
# ============================================================================

try:
    import mediapipe as mp
    HAS_MEDIAPIPE = True
except ImportError:
    HAS_MEDIAPIPE = False
    print("[INFO] MediaPipe not available, using OpenCV for face detection")


# ============================================================================
# 1. è¯åº“æ³¨å†Œæ¨¡å—
# ============================================================================

class LexiconRegistry:
    """
    è¯åº“åŠ è½½ä¸ç®¡ç†
    
    åŠŸèƒ½:
    -----
    - åŠ è½½ç¾å¦†é¢†åŸŸç›¸å…³çš„å„ç±»è¯åº“ï¼ˆæˆåˆ†ã€åŠŸæ•ˆã€å“ç±»ç­‰ï¼‰
    - åŠ è½½ç»“æ„åŒ–æ¨¡å¼ï¼ˆæ­£åˆ™è¡¨è¾¾å¼ï¼‰
    - ä¸ºjiebaåˆ†è¯æ³¨å†Œè‡ªå®šä¹‰è¯æ±‡
    
    è¯åº“ç±»å‹:
    ---------
    - ç¾å¦†çŸ¥è¯†: æˆåˆ†ã€åŠŸæ•ˆã€å“ç±»ã€è‚¤è´¨
    - å†…å®¹é£æ ¼: å£è¯­ã€æƒ…ç»ªã€çƒ­è¯
    - ç”¨æˆ·ä¸Šä¸‹æ–‡: äººç¾¤ã€åœºæ™¯ã€ç—›ç‚¹ã€é¢„ç®—
    - ç»“æ„æ¨¡å¼: å·å¬è¯­ã€ç”¨æ³•ã€æ€»ç»“ã€å¯¹æ¯”ç­‰
    """
    
    def __init__(self, base_dir='lexicons'):
        """
        åˆå§‹åŒ–è¯åº“
        
        å‚æ•°:
        -----
        base_dir: str
            è¯åº“æ ¹ç›®å½•è·¯å¾„
        """
        self.base_dir = base_dir
        
        # é›†åˆç±»è¯åº“
        self.ingredients = set()
        self.efficacy = set()
        self.product_categories = set()
        self.skin_types = set()
        self.colloquial = set()
        self.emotions = set()
        self.hot_words = set()
        self.audiences = set()
        self.pain_points = set()
        self.scenarios = set()
        self.budget_sensitive = set()
        self.search_keywords_global = set()
        
        # æ­£åˆ™æ¨¡å¼åˆ—è¡¨
        self.imperative_patterns = []
        self.budget_patterns = []
        self.usage_patterns = []
        self.summary_patterns = []
        self.comparison_patterns = []
        self.pain_solution_patterns = []
        
        # æ‰§è¡ŒåŠ è½½
        self._load_all()
        
    def _load_all(self):
        """åŠ è½½æ‰€æœ‰è¯åº“å’Œæ¨¡å¼"""
        print(f">>> æ­£åœ¨åŠ è½½è¯åº“ (base_dir={self.base_dir})...")
        
        try:
            # åŠ è½½æœ¯è¯­è¯åº“
            self._load_terms('beauty_knowledge/ingredients.json', self.ingredients)
            self._load_terms('beauty_knowledge/efficacy.json', self.efficacy)
            self._load_terms('beauty_knowledge/product_category.json', self.product_categories)
            self._load_terms('beauty_knowledge/skin_type.json', self.skin_types)
            self._load_terms('content_style/colloquial.json', self.colloquial)
            self._load_terms('content_style/emotion.json', self.emotions)
            self._load_terms('content_style/hotwords.json', self.hot_words)
            self._load_terms('user_context/audience.json', self.audiences)
            self._load_terms('user_context/painpoint.json', self.pain_points)
            self._load_terms('user_context/scenario.json', self.scenarios)
            self._load_terms('user_context/budget.json', self.budget_sensitive)
            
            # åŠ è½½æ­£åˆ™æ¨¡å¼
            self._load_simple_patterns('patterns/imperative_patterns.json', self.imperative_patterns)
            self._load_simple_patterns('user_context/budget.json', self.budget_patterns, key='price_patterns')
            self._load_structure_patterns('patterns/structure_patterns.json')
            self._load_search_keywords('search/search_keywords.json')
            
            # æ³¨å†Œåˆ°jiebaåˆ†è¯å™¨
            all_words = self.ingredients | self.efficacy | self.pain_points | self.hot_words
            for word in all_words:
                jieba.add_word(word)
                
            print(f"[âœ“] è¯åº“åŠ è½½å®Œæˆ: æˆåˆ†={len(self.ingredients)}, åŠŸæ•ˆ={len(self.efficacy)}")
            
        except Exception as e:
            print(f"[âœ—] è¯åº“åŠ è½½å¼‚å¸¸: {e}")
            import traceback
            traceback.print_exc()
    
    def _load_terms(self, rel_path, target_set):
        """åŠ è½½æ ‡å‡†æ ¼å¼çš„æœ¯è¯­è¯åº“"""
        path = os.path.join(self.base_dir, rel_path)
        if not os.path.exists(path):
            print(f"[WARN] æ–‡ä»¶ä¸å­˜åœ¨: {path}")
            return
        
        try:
            with open(path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                if 'terms' in data and isinstance(data['terms'], list):
                    for item in data['terms']:
                        if 'term' in item:
                            target_set.add(item['term'])
                        if 'synonyms' in item:
                            target_set.update(item['synonyms'])
        except Exception as e:
            print(f"[WARN] åŠ è½½å¤±è´¥ {rel_path}: {e}")
    
    def _load_simple_patterns(self, rel_path, target_list, key='patterns'):
        """åŠ è½½ç®€å•çš„æ­£åˆ™æ¨¡å¼åˆ—è¡¨"""
        path = os.path.join(self.base_dir, rel_path)
        if not os.path.exists(path):
            return
        
        try:
            with open(path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                patterns = data.get(key, [])
                for p in patterns:
                    if isinstance(p, dict) and 'regex' in p:
                        target_list.append(p['regex'])
                    elif isinstance(p, str):
                        target_list.append(p)
        except Exception as e:
            print(f"[WARN] åŠ è½½æ¨¡å¼å¤±è´¥ {rel_path}: {e}")
    
    def _load_structure_patterns(self, rel_path):
        """åŠ è½½ç»“æ„åŒ–æ¨¡å¼ï¼ˆå¤æ‚ç»“æ„ï¼‰"""
        path = os.path.join(self.base_dir, rel_path)
        if not os.path.exists(path):
            return
        
        try:
            with open(path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                root = data.get('patterns', {})
                
                def extract_regex(group_name):
                    result = []
                    items = root.get(group_name, [])
                    for item in items:
                        if 'regex' in item:
                            result.append(item['regex'])
                    return result
                
                self.usage_patterns = extract_regex('usage_method')
                self.summary_patterns = extract_regex('summary')
                self.comparison_patterns = extract_regex('comparison')
                self.pain_solution_patterns = extract_regex('painpoint_solution_effect')
                
        except Exception as e:
            print(f"[WARN] åŠ è½½ç»“æ„æ¨¡å¼å¤±è´¥: {e}")
    
    def _load_search_keywords(self, rel_path):
        """åŠ è½½æ£€ç´¢å…³é”®è¯"""
        path = os.path.join(self.base_dir, rel_path)
        if not os.path.exists(path):
            return
        
        try:
            with open(path, 'r', encoding='utf-8') as f:
                content = json.load(f)
                groups = content.get('groups', {})
                for group_val in groups.values():
                    if 'terms' in group_val:
                        for t in group_val['terms']:
                            self.search_keywords_global.add(t['term'])
                            if 'synonyms' in t:
                                self.search_keywords_global.update(t['synonyms'])
        except Exception as e:
            print(f"[WARN] åŠ è½½æ£€ç´¢è¯å¤±è´¥: {e}")


# ============================================================================
# 2. æ–‡æœ¬ç‰¹å¾æå–æ¨¡å—
# ============================================================================

class TextFeatureExtractor:
    """
    æ–‡æœ¬ç‰¹å¾æå–å™¨
    
    åŠŸèƒ½:
    -----
    ä»ç¬”è®°çš„æ ‡é¢˜ã€æ­£æ–‡ã€æ ‡ç­¾ä¸­æå–45ç»´æ–‡æœ¬ç‰¹å¾ï¼ŒåŒ…æ‹¬ï¼š
    - æ ‡é¢˜ç‰¹å¾ (7ç»´): é•¿åº¦ã€æ•°å­—ã€ç–‘é—®å¥ã€å…³é”®è¯è¦†ç›–ç­‰
    - æ­£æ–‡ç»“æ„ (9ç»´): é•¿åº¦ã€å¥æ•°ã€æ®µè½ã€åˆ—è¡¨ã€æ€»ç»“ç­‰
    - è¯­ä¹‰ç‰¹å¾ (24ç»´): çƒ­è¯ã€å£è¯­ã€æˆåˆ†ã€åŠŸæ•ˆã€äººç¾¤ç­‰
    - æ ‡ç­¾ç‰¹å¾ (5ç»´): æ•°é‡ã€ä¸€è‡´æ€§ã€å‚ç›´æ ‡ç­¾ç­‰
    """
    
    def __init__(self, lexicons):
        """
        åˆå§‹åŒ–æå–å™¨
        
        å‚æ•°:
        -----
        lexicons: LexiconRegistry
            å·²åŠ è½½çš„è¯åº“å®ä¾‹
        """
        self.lex = lexicons
    
    # ------------------------------------------------------------------------
    # å·¥å…·æ–¹æ³•
    # ------------------------------------------------------------------------
    
    def _count_hits(self, text, lexicon):
        """è®¡ç®—æ–‡æœ¬å‘½ä¸­è¯åº“çš„æ¬¡æ•°ï¼ˆå«é‡å¤ï¼‰"""
        return sum(1 for word in lexicon if word in text) if text else 0
    
    def _count_unique_hits(self, text, lexicon):
        """è®¡ç®—æ–‡æœ¬å‘½ä¸­è¯åº“çš„ä¸é‡å¤è¯æ•°"""
        return len(set(word for word in lexicon if word in text)) if text else 0
    
    def _check_regex(self, text, patterns):
        """æ£€æŸ¥æ–‡æœ¬æ˜¯å¦åŒ¹é…ä»»ä¸€æ­£åˆ™æ¨¡å¼"""
        if not text:
            return 0
        for pat in patterns:
            try:
                if re.search(pat, text, re.IGNORECASE):
                    return 1
            except re.error:
                continue
        return 0
    
    # ------------------------------------------------------------------------
    # ç‰¹å¾æå–æ¨¡å—
    # ------------------------------------------------------------------------
    
    def extract_title_features(self, row):
        """
        æå–æ ‡é¢˜ç‰¹å¾ (7ç»´)
        
        ç‰¹å¾åˆ—è¡¨:
        ---------
        0. title_len: æ ‡é¢˜é•¿åº¦
        1. title_number_flag: æ˜¯å¦åŒ…å«æ•°å­—
        2. title_question_flag: æ˜¯å¦ä¸ºç–‘é—®å¥
        3. title_keyword_cov: æ ¸å¿ƒå…³é”®è¯è¦†ç›–ç‡
        4. title_keyword_cnt: å…³é”®è¯å‘½ä¸­æ•°é‡
        5. title_keyword_pos_score: å…³é”®è¯ä½ç½®å¾—åˆ†
        6. title_readability_score: å¯è¯»æ€§å¾—åˆ†
        """
        title = str(row.get('title', ''))
        search_kw = str(row.get('search_keyword', ''))
        
        feats = {}
        feats['title_len'] = len(title)
        feats['title_number_flag'] = 1 if re.search(r'\d|[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]', title) else 0
        feats['title_question_flag'] = 1 if re.search(r'[?ï¼Ÿ]|æ€ä¹ˆ|å¦‚ä½•|å¥½ç”¨å—|ä»€ä¹ˆ|é¿é›·å—', title) else 0
        feats['title_keyword_cov'] = 1 if (search_kw and search_kw in title) else 0
        feats['title_keyword_cnt'] = self._count_hits(title, self.lex.search_keywords_global)
        
        # å…³é”®è¯ä½ç½®å¾—åˆ†
        if search_kw and search_kw in title:
            pos = title.find(search_kw)
            feats['title_keyword_pos_score'] = 1 - (pos / len(title))
        else:
            feats['title_keyword_pos_score'] = 0
        
        # å¯è¯»æ€§: ç¬¦å·å æ¯”è¶Šä½è¶Šå¥½
        symbol_cnt = len(re.findall(r'[^\w\s]', title))
        feats['title_readability_score'] = 1 - (symbol_cnt / (len(title) + 1))
        
        return feats
    
    def extract_content_features(self, row):
        """
        æå–æ­£æ–‡ç»“æ„ç‰¹å¾ (9ç»´)
        
        ç‰¹å¾åˆ—è¡¨:
        ---------
        7. content_len: æ­£æ–‡é•¿åº¦
        8. sentence_cnt: å¥å­æ•°
        9. avg_sentence_len: å¹³å‡å¥é•¿
        10. paragraph_cnt: æ®µè½æ•°
        11. list_structure_flag: æ˜¯å¦æœ‰åˆ—è¡¨ç»“æ„
        12. summary_flag: æ˜¯å¦æœ‰æ€»ç»“æ®µè½
        13. info_density_score: ä¿¡æ¯å¯†åº¦
        14. readability_score: å¯è¯»æ€§
        15. solution_pattern_flag: ç—›ç‚¹-æ–¹æ¡ˆ-æ•ˆæœç»“æ„
        """
        desc = str(row.get('desc', ''))
        feats = {}
        
        feats['content_len'] = len(desc)
        
        # å¥å­åˆ‡åˆ†
        sentences = [s for s in re.split(r'[ã€‚ï¼ï¼Ÿ.!?\n]', desc) if len(s.strip()) > 1]
        feats['sentence_cnt'] = len(sentences)
        feats['avg_sentence_len'] = np.mean([len(s) for s in sentences]) if sentences else 0
        
        # æ®µè½å’Œç»“æ„
        feats['paragraph_cnt'] = desc.count('\n') + 1
        list_matches = re.findall(r'(\d\.|[abcd]\.|â€¢|âœ”|âœ…|ğŸ‘‰|â‘ |â‘¡)', desc)
        feats['list_structure_flag'] = 1 if len(list_matches) >= 3 else 0
        
        # æ€»ç»“æ®µè½
        if self.lex.summary_patterns:
            feats['summary_flag'] = self._check_regex(desc, self.lex.summary_patterns)
        else:
            feats['summary_flag'] = self._check_regex(desc, [r'(æ€»ç»“|ç»¼ä¸Š|ç»“è®º|æœ€å|æ€»çš„æ¥è¯´)'])
        
        # ä¿¡æ¯å¯†åº¦ï¼ˆå®è¯æ¯”ä¾‹ï¼‰
        try:
            words = list(pseg.cut(desc))
            content_words = [w for w, flag in words if flag.startswith(('n', 'v', 'a'))]
            feats['info_density_score'] = len(content_words) / (len(words) + 1)
        except:
            feats['info_density_score'] = 0
        
        # å¯è¯»æ€§
        avg_len = feats['avg_sentence_len']
        feats['readability_score'] = max(0, min(1, 1 - (avg_len - 5) / 45))
        
        # ç—›ç‚¹-æ–¹æ¡ˆ-æ•ˆæœé“¾
        pat_flag = self._check_regex(desc, self.lex.pain_solution_patterns)
        if pat_flag:
            feats['solution_pattern_flag'] = 1
        else:
            has_pain = 1 if self._count_hits(desc, self.lex.pain_points) > 0 else 0
            has_eff = 1 if self._count_hits(desc, self.lex.efficacy) > 0 else 0
            feats['solution_pattern_flag'] = 1 if (has_pain and has_eff) else 0
        
        return feats
    
    def extract_semantic_features(self, row):
        """
        æå–è¯­ä¹‰ç‰¹å¾ (24ç»´)
        
        ç‰¹å¾åˆ—è¡¨:
        ---------
        16-19: çƒ­è¯ã€å£è¯­ã€Emojiã€æ ‡ç‚¹
        20-21: æ„Ÿå¹å·ã€é—®å·å æ¯”
        22-24: ç¬¬äºŒäººç§°ã€ç¥ˆä½¿å¥ã€æƒ…ç»ªå¼ºåº¦
        25-28: æˆåˆ†è¯ã€åŠŸæ•ˆè¯ï¼ˆæ•°é‡+å¤šæ ·æ€§ï¼‰
        29-30: è‚¤è´¨è¯ã€å“ç±»è¯
        31-32: ç”¨æ³•/æ­¥éª¤ã€å¯¹æ¯”ä¿¡æ¯
        33-35: äººç¾¤ã€åœºæ™¯ã€ç—›ç‚¹è¯
        36: ä»·æ ¼æ•æ„Ÿåº¦
        37-39: æ£€ç´¢è¯è¦†ç›–ã€å¯†åº¦
        """
        full_text = str(row.get('title', '')) + " " + str(row.get('desc', ''))
        feats = {}
        
        # å†…å®¹é£æ ¼
        feats['hotword_hit_rate'] = self._count_hits(full_text, self.lex.hot_words) / (len(full_text) / 100 + 1)
        feats['colloquial_ratio'] = self._count_hits(full_text, self.lex.colloquial) / (len(full_text) / 100 + 1)
        emoji_cnt = len(re.findall(r'\[.*?\]', full_text))
        feats['emoji_ratio'] = emoji_cnt / (len(full_text) + 1)
        
        # æ ‡ç‚¹ç¬¦å·
        punct_cnt = len(re.findall(r'[ï¼Œã€‚ï¼ï¼Ÿã€,!?]', full_text))
        feats['punctuation_density'] = punct_cnt / (len(full_text) + 1)
        feats['exclamation_ratio'] = (full_text.count('!') + full_text.count('ï¼')) / (len(full_text) + 1)
        feats['question_ratio'] = (full_text.count('?') + full_text.count('ï¼Ÿ')) / (len(full_text) + 1)
        
        # äº’åŠ¨é£æ ¼
        sec_person_words = ['ä½ ', 'ä½ ä»¬', 'å§å¦¹', 'å®å®', 'å¤§å®¶', 'é›†ç¾']
        feats['second_person_ratio'] = sum(full_text.count(w) for w in sec_person_words) / (len(full_text) / 100 + 1)
        feats['imperative_ratio'] = self._check_regex(full_text, self.lex.imperative_patterns)
        feats['sentiment_intensity'] = feats['exclamation_ratio'] * 100 + self._count_hits(full_text, self.lex.emotions)
        
        # ç¾å¦†ä¸“ä¸šçŸ¥è¯†
        feats['ingredient_cnt'] = self._count_hits(full_text, self.lex.ingredients)
        feats['ingredient_diversity'] = self._count_unique_hits(full_text, self.lex.ingredients)
        feats['efficacy_cnt'] = self._count_hits(full_text, self.lex.efficacy)
        feats['efficacy_diversity'] = self._count_unique_hits(full_text, self.lex.efficacy)
        feats['skin_type_cnt'] = self._count_hits(full_text, self.lex.skin_types)
        feats['product_category_cnt'] = self._count_hits(full_text, self.lex.product_categories)
        
        # å†…å®¹ç»“æ„
        feats['usage_method_flag'] = self._check_regex(full_text, self.lex.usage_patterns)
        if self.lex.comparison_patterns:
            feats['comparison_flag'] = self._check_regex(full_text, self.lex.comparison_patterns)
        else:
            feats['comparison_flag'] = 1 if re.search(r'(å¯¹æ¯”|åŒºåˆ«|PK|pk|èƒœå‡º|å‰å|å˜åŒ–)', full_text) else 0
        
        # ç”¨æˆ·å®šä½
        feats['audience_word_cnt'] = self._count_hits(full_text, self.lex.audiences)
        feats['scenario_word_cnt'] = self._count_hits(full_text, self.lex.scenarios)
        feats['painpoint_word_cnt'] = self._count_hits(full_text, self.lex.pain_points)
        
        # ä»·æ ¼æ•æ„Ÿ
        feats['budget_sensitivity_flag'] = 1 if (
            self._count_hits(full_text, self.lex.budget_sensitive) > 0 or
            self._check_regex(full_text, self.lex.budget_patterns)
        ) else 0
        
        # æ£€ç´¢è¯ç›¸å…³
        feats['search_keyword_cov'] = 1 if str(row.get('search_keyword')) in full_text else 0
        feats['search_keyword_cnt'] = self._count_hits(full_text, self.lex.search_keywords_global)
        feats['keyword_density'] = feats['search_keyword_cnt'] / (len(full_text) + 1)
        
        return feats
    
    def extract_tag_features(self, row):
        """
        æå–æ ‡ç­¾ç‰¹å¾ (5ç»´)
        
        ç‰¹å¾åˆ—è¡¨:
        ---------
        40. tag_content_consistency: æ ‡ç­¾ä¸æ­£æ–‡ä¸€è‡´æ€§
        41. tag_cnt: æ ‡ç­¾æ•°é‡
        42. vertical_tag_ratio: å‚ç›´æ ‡ç­¾å æ¯”
        43. generic_tag_ratio: æ³›æ ‡ç­¾å æ¯”
        44. tag_keyword_hit_cnt: æ ‡ç­¾å‘½ä¸­å…³é”®è¯æ•°
        """
        tags_str = str(row.get('tags', ''))
        tags = [t.strip().replace('#', '').replace('[è¯é¢˜]', '') 
                for t in re.split(r'[,ï¼Œ\s]', tags_str) if t.strip()]
        
        feats = {}
        
        # æ ‡ç­¾ä¸æ­£æ–‡ä¸€è‡´æ€§
        desc = str(row.get('desc', ''))
        overlap = sum(1 for t in tags if t in desc)
        feats['tag_content_consistency'] = overlap / len(tags) if tags else 0
        
        # æ ‡ç­¾ç»Ÿè®¡
        feats['tag_cnt'] = len(tags)
        feats['vertical_tag_ratio'] = 0  # è¯åº“ç¼ºå¤±ï¼Œä¿ç•™æ¥å£
        feats['generic_tag_ratio'] = 0   # è¯åº“ç¼ºå¤±ï¼Œä¿ç•™æ¥å£
        feats['tag_keyword_hit_cnt'] = sum(1 for t in tags if t in self.lex.search_keywords_global)
        
        return feats
    
    def extract(self, row):
        """
        æå–å•è¡Œçš„æ‰€æœ‰æ–‡æœ¬ç‰¹å¾
        
        å‚æ•°:
        -----
        row: pd.Series
            åŒ…å« title, desc, tags, search_keyword çš„æ•°æ®è¡Œ
        
        è¿”å›:
        -----
        dict: åŒ…å«45ç»´æ–‡æœ¬ç‰¹å¾çš„å­—å…¸
        """
        features = {}
        features.update(self.extract_title_features(row))
        features.update(self.extract_content_features(row))
        features.update(self.extract_semantic_features(row))
        features.update(self.extract_tag_features(row))
        return features


# ============================================================================
# 3. è§†è§‰ç‰¹å¾æå–æ¨¡å—
# ============================================================================

class VisualFeatureExtractor:
    """
    è§†è§‰ç‰¹å¾æå–å™¨
    
    åŠŸèƒ½:
    -----
    ä»ç¬”è®°å°é¢å›¾ç‰‡ä¸­æå–10ç»´è§†è§‰ç‰¹å¾ï¼ŒåŒ…æ‹¬ï¼š
    - å…‰å½±è‰²å½© (4ç»´): äº®åº¦ã€é¥±å’Œåº¦ã€å¯¹æ¯”åº¦ã€è‰²å½©ä¸°å¯Œåº¦
    - è´¨é‡é£æ ¼ (3ç»´): æ¸…æ™°åº¦ã€ç†µã€è§†è§‰å¤æ‚åº¦
    - äººåƒä¸»ä½“ (3ç»´): æ˜¯å¦æœ‰äººã€äººè„¸å æ¯”ã€äººè„¸æ•°é‡
    
    æŠ€æœ¯æ–¹æ¡ˆ:
    ---------
    - äººè„¸æ£€æµ‹: MediaPipe (ä¼˜å…ˆ) > OpenCV Haar Cascade (å¤‡é€‰)
    - è‰²å½©åˆ†æ: Hasler & SÃ¼sstrunk ç®—æ³•
    - æ¸…æ™°åº¦: Laplacian æ–¹å·®
    """
    
    def __init__(self, use_mediapipe=True):
        """
        åˆå§‹åŒ–æå–å™¨
        
        å‚æ•°:
        -----
        use_mediapipe: bool
            æ˜¯å¦å°è¯•ä½¿ç”¨MediaPipeï¼ˆæ›´å‡†ç¡®ä½†å¯èƒ½ä¸ç¨³å®šï¼‰
        """
        self.mp_face_detection = None
        self.face_cascade = None
        
        # å°è¯•åˆå§‹åŒ– MediaPipe
        if HAS_MEDIAPIPE and use_mediapipe:
            try:
                self.mp_face_detection = mp.solutions.face_detection.FaceDetection(
                    model_selection=1, min_detection_confidence=0.5
                )
            except Exception as e:
                print(f"[WARN] MediaPipeåˆå§‹åŒ–å¤±è´¥: {e}")
                self.mp_face_detection = None
        
        # å¤‡é€‰: OpenCV Haar Cascade
        if not self.mp_face_detection:
            cascade_path = "/opt/anaconda3/envs/ML/share/opencv4/haarcascades/haarcascade_frontalface_default.xml"
            if not os.path.exists(cascade_path):
                cascade_path = os.path.join(cv2.data.haarcascades, 'haarcascade_frontalface_default.xml')
            
            if os.path.exists(cascade_path):
                self.face_cascade = cv2.CascadeClassifier(cascade_path)
            else:
                print(f"[WARN] äººè„¸æ£€æµ‹æ¨¡å‹æœªæ‰¾åˆ°: {cascade_path}")
    
    def extract(self, image_path):
        """
        æå–å•å¼ å›¾ç‰‡çš„è§†è§‰ç‰¹å¾
        
        å‚æ•°:
        -----
        image_path: str
            å›¾ç‰‡æ–‡ä»¶è·¯å¾„
        
        è¿”å›:
        -----
        dict: åŒ…å«10ç»´è§†è§‰ç‰¹å¾çš„å­—å…¸
        """
        # ç‰¹å¾é»˜è®¤å€¼
        features = {
            'brightness_mean': 0.0,
            'saturation_mean': 0.0,
            'contrast_score': 0.0,
            'colorfulness_score': 0.0,
            'sharpness_score': 0.0,
            'entropy_score': 0.0,
            'visual_complexity': 0.0,
            'human_present': 0,
            'face_area_ratio': 0.0,
            'face_count': 0
        }
        
        # æ£€æŸ¥æ–‡ä»¶å­˜åœ¨æ€§
        if not os.path.exists(image_path):
            return features
        
        # è¯»å–å›¾ç‰‡
        img = cv2.imread(image_path)
        if img is None:
            return features
        
        try:
            height, width = img.shape[:2]
            total_pixels = height * width
            
            # é¢„å¤„ç†
            gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
            hsv_img = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
            h, s, v = cv2.split(hsv_img)
            
            # ----------------------------------------------------------------
            # æ¨¡å—A: å…‰å½±è‰²å½©ç‰¹å¾
            # ----------------------------------------------------------------
            
            features['brightness_mean'] = np.mean(v) / 255.0
            features['saturation_mean'] = np.mean(s) / 255.0
            features['contrast_score'] = np.std(gray_img) / 128.0
            
            # è‰²å½©ä¸°å¯Œåº¦ (Hasler & SÃ¼sstrunk ç®—æ³•)
            B, G, R = cv2.split(img.astype("float"))
            rg = np.absolute(R - G)
            yb = np.absolute(0.5 * (R + G) - B)
            std_root = np.sqrt((np.std(rg) ** 2) + (np.std(yb) ** 2))
            mean_root = np.sqrt((np.mean(rg) ** 2) + (np.mean(yb) ** 2))
            features['colorfulness_score'] = std_root + (0.3 * mean_root)
            
            # ----------------------------------------------------------------
            # æ¨¡å—B: è´¨é‡ä¸å¤æ‚åº¦ç‰¹å¾
            # ----------------------------------------------------------------
            
            # æ¸…æ™°åº¦ (Laplacianæ–¹å·®)
            laplacian_var = cv2.Laplacian(gray_img, cv2.CV_64F).var()
            features['sharpness_score'] = math.log(laplacian_var + 1)
            
            # è§†è§‰å¤æ‚åº¦ (è¾¹ç¼˜å¯†åº¦)
            edges = cv2.Canny(gray_img, 100, 200)
            features['visual_complexity'] = np.count_nonzero(edges) / total_pixels
            
            # å›¾åƒç†µ
            hist = cv2.calcHist([gray_img], [0], None, [256], [0, 256])
            hist_norm = hist.ravel() / hist.sum()
            logs = np.log2(hist_norm + 1e-7)
            features['entropy_score'] = -1 * (hist_norm * logs).sum()
            
            # ----------------------------------------------------------------
            # æ¨¡å—C: äººåƒä¸»ä½“ç‰¹å¾
            # ----------------------------------------------------------------
            
            face_area = 0.0
            face_count = 0
            
            if HAS_MEDIAPIPE and self.mp_face_detection:
                # MediaPipe æ£€æµ‹
                rgb_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                results = self.mp_face_detection.process(rgb_img)
                
                if results.detections:
                    face_count = len(results.detections)
                    for detection in results.detections:
                        bboxC = detection.location_data.relative_bounding_box
                        face_area += (bboxC.width * bboxC.height)
            
            elif self.face_cascade:
                # OpenCV Haar Cascade æ£€æµ‹
                faces = self.face_cascade.detectMultiScale(
                    gray_img, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30)
                )
                face_count = len(faces)
                for (x, y, w, h) in faces:
                    face_area += (w * h) / total_pixels
            
            features['human_present'] = 1 if face_count > 0 else 0
            features['face_count'] = face_count
            features['face_area_ratio'] = min(face_area, 1.0)
        
        except Exception as e:
            print(f"[WARN] å›¾ç‰‡å¤„ç†å¤±è´¥ {image_path}: {e}")
        
        return features


# ============================================================================
# 4. æ‰¹é‡å¤„ç†åè°ƒå™¨
# ============================================================================

class BatchProcessor:
    """
    æ‰¹é‡ç‰¹å¾æå–åè°ƒå™¨
    
    åŠŸèƒ½:
    -----
    1. è¯»å–CSVæ•°æ®
    2. åŠ è½½è¯åº“
    3. æ‰¹é‡æå–æ–‡æœ¬ç‰¹å¾
    4. æŸ¥æ‰¾å¯¹åº”å›¾ç‰‡å¹¶æå–è§†è§‰ç‰¹å¾
    5. åˆå¹¶æ‰€æœ‰ç‰¹å¾å¹¶ä¿å­˜
    """
    
    def __init__(self, 
                 input_csv='data/data_with_label.csv',
                 output_csv='data/data_with_full_features.csv',
                 image_dir='image',
                 lexicon_dir='lexicons',
                 use_mediapipe=False):
        """
        åˆå§‹åŒ–æ‰¹å¤„ç†å™¨
        
        å‚æ•°:
        -----
        input_csv: str
            è¾“å…¥CSVè·¯å¾„
        output_csv: str
            è¾“å‡ºCSVè·¯å¾„
        image_dir: str
            å›¾ç‰‡ç›®å½•è·¯å¾„
        lexicon_dir: str
            è¯åº“ç›®å½•è·¯å¾„
        use_mediapipe: bool
            è§†è§‰æå–æ˜¯å¦ä½¿ç”¨MediaPipe
        """
        self.input_csv = input_csv
        self.output_csv = output_csv
        self.image_dir = image_dir
        self.lexicon_dir = lexicon_dir
        self.use_mediapipe = use_mediapipe
        
        # å…¼å®¹ä»srcç›®å½•è¿è¡Œ
        self._adjust_paths()
        
        # æå–å™¨å®ä¾‹ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
        self.lexicons = None
        self.text_extractor = None
        self.visual_extractor = None
        self.image_map = {}
    
    def _adjust_paths(self):
        """è‡ªåŠ¨è°ƒæ•´è·¯å¾„ï¼ˆå…¼å®¹ä»src/ç›®å½•è¿è¡Œï¼‰"""
        if not os.path.exists(self.input_csv):
            self.input_csv = os.path.join('..', self.input_csv)
        if not os.path.exists(self.image_dir):
            self.image_dir = os.path.join('..', self.image_dir)
        if not os.path.exists(self.lexicon_dir):
            self.lexicon_dir = os.path.join('..', self.lexicon_dir)
    
    def _build_image_index(self):
        """æ„å»º note_id -> image_path çš„æ˜ å°„"""
        print(f"\n>>> æ‰«æå›¾ç‰‡ç›®å½•: {self.image_dir}")
        
        for root, _, files in os.walk(self.image_dir):
            for file in files:
                if file.lower().endswith(('.jpg', '.jpeg', '.png', '.webp')):
                    note_id = os.path.splitext(file)[0]
                    self.image_map[note_id] = os.path.join(root, file)
        
        print(f"[âœ“] æ‰¾åˆ° {len(self.image_map)} å¼ å›¾ç‰‡")
    
    def run(self):
        """æ‰§è¡Œå®Œæ•´çš„æ‰¹é‡æå–æµç¨‹"""
        print("=" * 70)
        print("æ‰¹é‡ç‰¹å¾æå–å™¨ - æ–‡æœ¬+è§†è§‰")
        print("=" * 70)
        
        # ====================================================================
        # Step 1: æ•°æ®åŠ è½½
        # ====================================================================
        print(f"\n[1/5] è¯»å–æ•°æ®: {self.input_csv}")
        if not os.path.exists(self.input_csv):
            print(f"[âœ—] æ–‡ä»¶ä¸å­˜åœ¨: {self.input_csv}")
            return
        
        df = pd.read_csv(self.input_csv)
        print(f"[âœ“] æ•°æ®è¡Œæ•°: {len(df)}")
        
        # ====================================================================
        # Step 2: åˆå§‹åŒ–è¯åº“å’Œæå–å™¨
        # ====================================================================
        print(f"\n[2/5] åˆå§‹åŒ–è¯åº“å’Œæå–å™¨")
        self.lexicons = LexiconRegistry(base_dir=self.lexicon_dir)
        self.text_extractor = TextFeatureExtractor(self.lexicons)
        self.visual_extractor = VisualFeatureExtractor(use_mediapipe=self.use_mediapipe)
        print("[âœ“] æå–å™¨å°±ç»ª")
        
        # ====================================================================
        # Step 3: æ–‡æœ¬ç‰¹å¾æå–
        # ====================================================================
        print(f"\n[3/5] æå–æ–‡æœ¬ç‰¹å¾ (45ç»´)")
        text_features = []
        
        for idx, row in tqdm(df.iterrows(), total=len(df), desc="æ–‡æœ¬ç‰¹å¾"):
            feats = self.text_extractor.extract(row)
            text_features.append(feats)
        
        text_feat_df = pd.DataFrame(text_features)
        print(f"[âœ“] æ–‡æœ¬ç‰¹å¾æå–å®Œæˆï¼Œç»´åº¦: {text_feat_df.shape[1]}")
        
        # ====================================================================
        # Step 4: è§†è§‰ç‰¹å¾æå–
        # ====================================================================
        print(f"\n[4/5] æå–è§†è§‰ç‰¹å¾ (10ç»´)")
        self._build_image_index()
        
        visual_features = []
        success_count = 0
        missing_count = 0
        
        for idx, row in tqdm(df.iterrows(), total=len(df), desc="è§†è§‰ç‰¹å¾"):
            note_id = str(row['note_id'])
            
            if note_id in self.image_map:
                img_path = self.image_map[note_id]
                try:
                    feats = self.visual_extractor.extract(img_path)
                    visual_features.append(feats)
                    success_count += 1
                except Exception as e:
                    # å‡ºé”™æ—¶ä½¿ç”¨é»˜è®¤å€¼
                    visual_features.append(self.visual_extractor.extract(""))
            else:
                # æœªæ‰¾åˆ°å›¾ç‰‡æ—¶ä½¿ç”¨é»˜è®¤å€¼
                visual_features.append(self.visual_extractor.extract(""))
                missing_count += 1
        
        visual_feat_df = pd.DataFrame(visual_features)
        print(f"[âœ“] è§†è§‰ç‰¹å¾æå–å®Œæˆ")
        print(f"    æˆåŠŸ: {success_count} å¼  | ç¼ºå¤±: {missing_count} å¼ ")
        
        # ====================================================================
        # Step 5: åˆå¹¶å¹¶ä¿å­˜
        # ====================================================================
        print(f"\n[5/5] åˆå¹¶ç‰¹å¾å¹¶ä¿å­˜")
        
        # ä¿ç•™å…ƒä¿¡æ¯åˆ—
        meta_cols = ['note_id', 'title', 'hot_level', 'search_keyword']
        meta_df = df[meta_cols]
        
        # åˆå¹¶æ‰€æœ‰ç‰¹å¾
        result_df = pd.concat([meta_df, text_feat_df, visual_feat_df], axis=1)
        
        # ä¿å­˜
        result_df.to_csv(self.output_csv, index=False, encoding='utf-8-sig')
        
        print(f"[âœ“] ä¿å­˜å®Œæˆ: {self.output_csv}")
        print(f"[âœ“] æœ€ç»ˆç»´åº¦: {result_df.shape} (è¡ŒÃ—åˆ—)")
        print(f"[âœ“] ç‰¹å¾åˆ—æ•°: {len(text_feat_df.columns) + len(visual_feat_df.columns)}")
        
        print("\n" + "=" * 70)
        print("âœ¨ æ‰¹é‡æå–å®Œæˆï¼")
        print("=" * 70)


# ============================================================================
# 5. ä¸»å‡½æ•°
# ============================================================================

def main():
    """ä¸»å…¥å£å‡½æ•°"""
    processor = BatchProcessor(
        input_csv='data/data_with_label.csv',
        output_csv='data/data_with_full_features.csv',
        image_dir='image',
        lexicon_dir='lexicons',
        use_mediapipe=False  # å»ºè®®å…³é—­ä»¥æé«˜ç¨³å®šæ€§
    )
    processor.run()


if __name__ == "__main__":
    main()

